{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of reminders about how to use Jupyter Notebook: \n",
    "- Hit Shift+Enter to run code and finish typing in Markdown\n",
    "- You should not have to change much code here, but be careful to document carefully if you do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary packages into Python. If there are any you don't have, type \"!pip install package-name\" above the import statements and it should install that package. You should only have to do that once, so you can delete it right afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create an empty documents list. This will store all of the documents that you want to assign clusters to. We've also created a function called \"preproces\", which reads in a text file with documents on *separate lines*, so if you need to read in a text file without documents on separate lines, you need to either alter the code below or alter your input text file to separate documents by line. If you want to read the whole text file in as a \"document\", change the \"readlines()\" to \"read\", alter the strip function to strip that document rather than using the for loop, and get rid of line 7, which currently loops through the different lines in one file to add them to the documents list (remember to change documents.append(x) to documents.append(filename)). \n",
    "\n",
    "After this step, we read in the text files for this example. To run this on your own data, make sure to store your own text files in the same folder as this notebook and simply replace the name of the file in the preprocess() function below like I have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "def preprocess(file):\n",
    "    with open (file, \"r\",encoding=\"utf-8\") as myfile:\n",
    "        filename=myfile.readlines()\n",
    "        filename = [x.strip() for x in filename]\n",
    "        for x in filename:\n",
    "            documents.append(x)\n",
    "          \n",
    "preprocess(\"Obama_town_hall_pre.txt\")\n",
    "preprocess(\"Obama_town_hall_post.txt\")\n",
    "preprocess(\"Bush_town_hall_pre.txt\")\n",
    "preprocess(\"Bush_town_hall_post.txt\")\n",
    "preprocess(\"Clinton_town_hall_pre.txt\")\n",
    "preprocess(\"Clinton_town_hall_post.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the sklearn TfidfVectorizer to get the term frequency inverse document frequency (a measure of relative frequency which compares how often a term was used in one document versus all the other documents). It also runs the list of documents through a stop word list and does much of the preprocessing (stemming etc.) for us in this one line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we assign the number of clusters we want to run using the \"true_k\" variable below. We let sklearn know that we're going to be using the KMeans clustering algorithm and pass through several parameters. The most important is n_clusters, which is the number of clusters we assigned in the previous line of code. One important bit of information to get out of this model is the model labels, which tell you which document got assigned to which cluster. You can uncomment out \"print(model.labels)\" to see a list of labels. \n",
    "\n",
    "Using this information for clusters, which are in the same order as documents, you can create a dictionary called \"doc\" to store the document as a key with the value of the cluster. Uncomment out \"print(docs)\" to see the results of the for loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 10\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "#print(model.labels_)\n",
    "\n",
    "docs = {}\n",
    "for i in range(0,len(documents)):\n",
    "    docs[documents[i]] = model.labels_[i]\n",
    "\n",
    "#print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below prints the top terms per cluster. This gives you an idea of the semantic content per cluster. You can also uncomment out the last line in this block of code to print the sorted cluster labels, which can give you an idea of which clusters are most numerous in your data -- if you see a lot of the same cluster, it may be a good idea to change your true_k value above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 1:\n",
      " president\n",
      " let\n",
      " state\n",
      " did\n",
      " right\n",
      " mr\n",
      " better\n",
      " vice\n",
      " thing\n",
      " say\n",
      "\n",
      "Cluster 2:\n",
      " people\n",
      " thank\n",
      " tax\n",
      " said\n",
      " need\n",
      " time\n",
      " ll\n",
      " way\n",
      " work\n",
      " world\n",
      "\n",
      "Cluster 3:\n",
      " going\n",
      " tax\n",
      " taxes\n",
      " jobs\n",
      " say\n",
      " tell\n",
      " says\n",
      " people\n",
      " make\n",
      " sure\n",
      "\n",
      "Cluster 4:\n",
      " make\n",
      " want\n",
      " governor\n",
      " sure\n",
      " romney\n",
      " point\n",
      " just\n",
      " people\n",
      " plan\n",
      " jobs\n",
      "\n",
      "Cluster 5:\n",
      " believe\n",
      " strongly\n",
      " ought\n",
      " people\n",
      " need\n",
      " local\n",
      " country\n",
      " say\n",
      " everybody\n",
      " schools\n",
      "\n",
      "Cluster 6:\n",
      " don\n",
      " think\n",
      " want\n",
      " really\n",
      " government\n",
      " like\n",
      " federal\n",
      " right\n",
      " word\n",
      " role\n",
      "\n",
      "Cluster 7:\n",
      " ve\n",
      " got\n",
      " make\n",
      " sure\n",
      " years\n",
      " energy\n",
      " people\n",
      " said\n",
      " plan\n",
      " president\n",
      "\n",
      "Cluster 8:\n",
      " think\n",
      " people\n",
      " important\n",
      " right\n",
      " american\n",
      " economy\n",
      " way\n",
      " ought\n",
      " grows\n",
      " going\n",
      "\n",
      "Cluster 9:\n",
      " just\n",
      " years\n",
      " health\n",
      " care\n",
      " let\n",
      " people\n",
      " 12\n",
      " ago\n",
      " insurance\n",
      " ve\n",
      "\n",
      "Cluster 10:\n",
      " senator\n",
      " mccain\n",
      " dole\n",
      " agree\n",
      " billion\n",
      " mentioned\n",
      " know\n",
      " going\n",
      " suggested\n",
      " earmarks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % (i+1),)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind],)\n",
    "    print()\n",
    "#print(sorted(model.labels_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
