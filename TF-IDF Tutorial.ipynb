{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of reminders about how to use Jupyter Notebook: \n",
    "- Hit Shift+Enter to run code and finish typing in Markdown\n",
    "- You should not have to change much code here, but be careful to document carefully if you do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary packages into Python. If there are any you don't have, type \"!pip install package-name\" above the import statements and it should install that package. You should only have to do that once, so you can delete it right afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial compares a set of three debates for each election for 6 presidential elections, for three presidents. It takes the first set of debates for a given president and compares it to the second set of debates for that president. However, this code can be adapted to do any comparison of any two (or more) documents. It measure \"Term Frequency Inverse Document Frequency\", which is a measure of frequency of a term in a document relative to all other documents in that set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading text files into a doc list. If you have more than two documents, you may consider creating a loop or a function to read them in. Since we're using a doc list for the rest of the steps, you can add as many docs as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = []\n",
    "\n",
    "with open (\"Clinton_term_one.txt\", \"r\",encoding=\"utf-8\") as myfile:\n",
    "    term_one=myfile.read().replace('\\n', ' ')\n",
    "    doc.append(term_one)\n",
    "\n",
    "with open (\"Clinton_term_two.txt\", \"r\",encoding=\"utf-8\") as myfile:\n",
    "    term_two=myfile.read().replace('\\n', ' ')\n",
    "    doc.append(term_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to get rid of punctuation and tokenize the documents - that is, break them into individual words. Uncomment \"#print(term_vec[0])\" to see the results of this preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation, then tokenize documents\n",
    "\n",
    "punc = re.compile( '[%s]' % re.escape( string.punctuation ) )\n",
    "term_vec = [ ]\n",
    "stop_words = nltk.corpus.stopwords.words( 'english' )\n",
    "stop_words = stop_words.append([\"youd\",\"5000\",\"300\",\"95\",\"”\",\"7\",\"wasnt\"])\n",
    "\n",
    "for d in doc:\n",
    "    d = d.lower()\n",
    "    d = punc.sub( '', d )\n",
    "    term_vec.append( nltk.word_tokenize( d ) )\n",
    "    \n",
    "#print(term_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords from the docs, i.e. unnecessary words that don't provide much information. To see the results of this processing, uncomment the print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from term vectors\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words( 'english' )\n",
    "term_list = []\n",
    "additional_words = [\"youd\",\"5000\",\"300\",\"95\",\"”\",\"7\",\"wasnt\"]\n",
    "#doc_list = []\n",
    "for doc in term_vec:\n",
    "    for term in doc:\n",
    "        if term in stop_words:\n",
    "            doc.remove(term)\n",
    "#print(term_vec[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the Porter stemmer to stem the words so that we don't get redundant information - we only need the roots of the words, like \"lead\", \"leader\", \"leadership\" will all be \"lead\" for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter stem remaining terms\n",
    "\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "for i in range( 0, len( term_vec ) ):\n",
    "    for j in range( 0, len( term_vec[ i ] ) ):\n",
    "        term_vec[ i ][ j ] = porter.stem( term_vec[ i ][ j ] )\n",
    "\n",
    "#print(term_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the gensim dictionary function to convert the term vectors into a gensim dictionary, which assigns an index to each unique word in term_vec. Below that we create a bag of words model (a model for text which ignores word order and context), and finally we create a tfidf matrix of the term frequency inverse document frequency values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Convert term vectors into gensim dictionary\n",
    "\n",
    "dict = gensim.corpora.Dictionary(term_vec)\n",
    "\n",
    "corp = []\n",
    "for i in range(0, len(term_vec)):\n",
    "    corp.append( dict.doc2bow( term_vec[ i ] ) )\n",
    "\n",
    "#  Create TFIDF vectors based on term vectors bag-of-word corpora\n",
    "\n",
    "tfidf_model = gensim.models.TfidfModel( corp )\n",
    "\n",
    "tfidf = [ ]\n",
    "for i in range( 0, len( corp ) ):\n",
    "    tfidf.append( tfidf_model[ corp[ i ] ] )\n",
    "\n",
    "dictionary = dict.token2id\n",
    "#print(tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the numbers you generated in the TFIDF list up there correspond to a measure of frequency of occurrences in *this* document compared to other documents. See which words show up in one document that don't in the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0 terms of note:\n",
      "trickledown\n",
      "challeng\n",
      "bush\n",
      "courag\n",
      "experi\n",
      "fourth\n",
      "200000\n",
      "middleclass\n",
      "corpor\n",
      "research\n",
      "wealthi\n",
      "statement\n",
      "150000\n",
      "engag\n",
      "technolog\n",
      "mobil\n",
      "arm\n",
      "applaud\n",
      "quickli\n",
      "execut\n",
      "reinvest\n",
      "low\n",
      "near\n",
      "rebuild\n",
      "strength\n",
      "statu\n",
      "surplu\n",
      "involv\n",
      "medicin\n",
      "oregon\n",
      "ceil\n",
      "wors\n",
      "oversea\n",
      "theori\n",
      "format\n",
      "again\n",
      "wait\n",
      "down\n",
      "influenc\n",
      "\n",
      "Doc 1 terms of note:\n",
      "dole\n",
      "took\n",
      "peac\n",
      "protect\n",
      "environ\n",
      "childrear\n",
      "ban\n",
      "bridg\n",
      "young\n",
      "tobacco\n",
      "sale\n",
      "focu\n",
      "270\n",
      "associ\n",
      "scheme\n",
      "financ\n",
      "alway\n",
      "prove\n",
      "led\n",
      "border\n",
      "penalti\n",
      "effect\n",
      "east\n",
      "becam\n",
      "cuba\n",
      "whatev\n",
      "san\n",
      "diego\n",
      "partisan\n",
      "depart\n",
      "level\n",
      "toxic\n",
      "youll\n",
      "stronger\n",
      "left\n",
      "sell\n",
      "qualifi\n",
      "religi\n",
      "up\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for key,val in dictionary.items():\n",
    "#    if val == 121:\n",
    "#        print(key)\n",
    "\n",
    "for i in range(0,len(tfidf)):\n",
    "    sorted_values = sorted(tfidf[i], key=lambda x: x[1])\n",
    "#term_one_tf = sorted(tfidf[0], key=lambda x: x[1])\n",
    "    first = (len(sorted_values))-39\n",
    "    top_40 = sorted_values[first:]\n",
    "    #print(top_40)\n",
    "    values =  [int(i[0]) for i in top_40]\n",
    "    print(\"Doc\",str(i),\"terms of note:\")\n",
    "    for key,val in dictionary.items():\n",
    "        if val in values:\n",
    "            print(key)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
